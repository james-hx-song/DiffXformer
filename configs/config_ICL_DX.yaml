max_learning_rate: 1.28e-4
min_learning_rate: 5.12e-6
warmup_steps: 1000
architecture: DiffFormer
dataset: ICL
max_iters: 40000
batch_size: 16
save_every: 2000
eval_every: 2000
num_train_examples: 50000
num_val_examples: 500
num_test_examples: 500
sequence_length: 20
d_input: 10
size: 17M
pretrain: True
work_dir: DiffFormer_ICL
tokenizer_name: "HuggingFaceTB/SmolLM-135M"
