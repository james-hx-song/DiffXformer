max_learning_rate: 1.28e-4
min_learning_rate: 5.12e-6
warmup_steps: 1000
architecture: TransFormer
dataset: FinQA
max_iters: 40000
batch_size: 16
save_every: 4000
eval_every: 4000
size: 17M
pretrain: True
train_file: third_party/FinQA/dataset/train.json
val_file: third_party/FinQA/dataset/dev.json
test_file: third_party/FinQA/dataset/test.json
work_dir: TransFormer_FinQA
tokenizer_name: "HuggingFaceTB/SmolLM-135M"
