max_learning_rate: 1.28e-4
min_learning_rate: 5.12e-6
warmup_steps: 0
architecture: TransFormer
dataset: LogiQA
max_iters: 20000
batch_size: 8
save_every: 4000
eval_every: 4000
size: 122M
pretrain: True
train_file: third_party/LogiQA-dataset/Train.txt
val_file: third_party/LogiQA-dataset/Eval.txt
test_file: third_party/LogiQA-dataset/Test.txt
work_dir: TransFormer_LogiQA_with_pretrained
tokenizer_name: "HuggingFaceTB/SmolLM-135M"
