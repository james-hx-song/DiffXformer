max_learning_rate: 1.28e-4
min_learning_rate: 5.12e-6
warmup_steps: 1000
architecture: TransFormer
dataset: MSMARCO
max_iters: 40000
batch_size: 16
save_every: 4000
eval_every: 4000
size: 17M
pretrain: True
train_file: third_party/MSMARCO/train_v2.1.json
val_file: third_party/MSMARCO/dev_v2.1.json
test_file: third_party/MSMARCO/eval_v2.1_public.json
work_dir: TransFormer_MSMARCO
tokenizer_name: "HuggingFaceTB/SmolLM-135M"
